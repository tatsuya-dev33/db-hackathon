# Databricks notebook source
# MAGIC %pip install mlflow==2.10.1 lxml==4.9.3 transformers==4.30.2 langchain==0.1.5 databricks-vectorsearch==0.22 python-dotenv streamlit
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md
# MAGIC ## Import library and set keys

# COMMAND ----------

import pandas as pd
from dbruntime.databricks_repl_context import get_context
import os
from pyspark.sql import SparkSession
from transformers import AutoTokenizer, OpenAIGPTTokenizer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pyspark.sql.functions import col, udf, length, pandas_udf
import pyspark.sql.functions as F
import mlflow.deployments
from dotenv import load_dotenv
import time
from databricks.vector_search.client import VectorSearchClient
from langchain_community.vectorstores import DatabricksVectorSearch
from langchain_community.embeddings import DatabricksEmbeddings
from databricks.sdk import WorkspaceClient
import databricks.sdk.service.catalog as c
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatDatabricks
from mlflow.models import infer_signature
import mlflow
import langchain
from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedModelInput, ServedModelInputWorkloadSize
from mlflow import MlflowClient

load_dotenv(".env")
SERVING_ENDPOINT_NAME = os.getenv("SERVING_ENDPOINT_NAME")
SECRET_SCOPE_NAME = os.getenv("SECRET_SCOPE_NAME")
SECRET_SCOPE_KEY = os.getenv("SECRET_SCOPE_KEY")
STREAMLIT_FILE = os.getenv("STREAMLIT_FILE")
EMBEDDING_ENDPOINT_NAME = os.getenv("EMBEDDING_ENDPOINT_NAME")
VECTOR_SEARCH_ENDPOINT_NAME = os.getenv("VECTOR_SEARCH_ENDPOINT_NAME")
catalog = os.getenv("catalog")
db = os.getenv("db")
LLM_ENDPOINT_NAME = os.getenv("LLM_ENDPOINT_NAME")


# COMMAND ----------

# MAGIC %md
# MAGIC # Prepare data

# COMMAND ----------

def remove_newlines(pandas_df):
    # delete line break
    cleaned_df = pandas_df.replace('\n', '', regex=True)
    cleaned_df = cleaned_df.replace('</w>', '', regex=True)
    return cleaned_df

df = pd.read_csv("copen_manual_RAG_file2.csv",engine="python", encoding="utf-8")
df = remove_newlines(df)

# COMMAND ----------

def pandas_to_spark(pandas_df):
    # create SparkSession
    spark = SparkSession.builder.getOrCreate()
    # change Pandas-DataFrame to Spark-DataFrame
    spark_df = spark.createDataFrame(pandas_df)
    return spark_df
    
doc_articles = pandas_to_spark(df)
doc_articles.write.mode('overwrite').saveAsTable("raw_documentation")

# COMMAND ----------

# MAGIC %md
# MAGIC ## split text into chunks

# COMMAND ----------

def split_text_into_chunks(text, min_chunk_size=20, max_chunk_size=3000):
    chunks = []
    current_chunk = ""
    for sentence in text.split(". "):
        # add sentence to current chunk
        current_chunk += sentence + ". "
        # create new chunk if chuck goes over max_chunk_size
        if len(current_chunk) > max_chunk_size:
            chunks.append(current_chunk.strip())
            current_chunk = ""
    # append last chunk
    if current_chunk:
        chunks.append(current_chunk.strip())
    # delete when chunk is too small
    chunks = [chunk for chunk in chunks if len(chunk) >= min_chunk_size]
    return chunks

# COMMAND ----------

max_chunk_size = 500

tokenizer = OpenAIGPTTokenizer.from_pretrained("openai-gpt")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=max_chunk_size, chunk_overlap=50)

# COMMAND ----------

# MAGIC %sql
# MAGIC --you need to enable Change Data Feed on the table in order to create an index.
# MAGIC CREATE TABLE IF NOT EXISTS copen_documentation_new_data (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   URL STRING,
# MAGIC   page STRING,
# MAGIC   file_1 STRING,
# MAGIC   file_2 STRING,
# MAGIC   file_3 STRING,
# MAGIC   file_4 STRING,
# MAGIC   file_5 STRING,
# MAGIC   file_6 STRING,
# MAGIC   file_7 STRING,
# MAGIC   file_8 STRING,
# MAGIC   file_9 STRING,
# MAGIC   file_10 STRING,
# MAGIC   content STRING
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true); 

# COMMAND ----------

# UDF for splitting text into chunks
@pandas_udf("array<string>")
def parse_and_split(docs: pd.Series) -> pd.Series:
    return docs.apply(split_text_into_chunks)
    
(spark.table("raw_documentation")
      .filter('text is not null')
      .withColumn('content', F.explode(parse_and_split('text')))
      .drop("text")
      .drop("big_category")
      .drop("small_category")
      .drop("detail")
      .write.mode('overwrite')
      .saveAsTable("copen_documentation_new_data"))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 

# COMMAND ----------

# MAGIC %md
# MAGIC ## create endpoint for vector search

# COMMAND ----------

vsc = VectorSearchClient()

try:
    endpoints = vsc.list_endpoints().get('endpoints', [])
    endpoint_names = [e['name'] for e in endpoints]
    
    if VECTOR_SEARCH_ENDPOINT_NAME not in endpoint_names:
        vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type="STANDARD")

except Exception as e:
    # Temp fix for potential REQUEST_LIMIT_EXCEEDED issue
    if "REQUEST_LIMIT_EXCEEDED" in str(e):
        print("WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. The demo will consider it exists")
        # Assuming that we consider the endpoint exists in this case
        pass
    else:
        raise e

# loop for check endpoint status
for i in range(180):
    try:
        # get endpoint information
        endpoint = vsc.get_endpoint(VECTOR_SEARCH_ENDPOINT_NAME)
    except Exception as e:
        # REQUEST_LIMIT_EXCEEDED error
        if "REQUEST_LIMIT_EXCEEDED" in str(e):
            print("WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. Please manually check your endpoint status")
            break
        else:
            raise e

    # get endpoint information
    status = endpoint.get("endpoint_status", endpoint.get("status"))["state"].upper()

    # when the endpoint is online
    if "ONLINE" in status:
        print(f"Endpoint is ready.")
        break
    # when the endpoint is PROVISIONING or first 6time
    elif "PROVISIONING" in status or i < 6:
        if i % 20 == 0: 
            print(f"Waiting for endpoint to be ready, this can take a few min... {endpoint}")
        time.sleep(10)
    else:
        raise Exception(f'''Error with the endpoint {VECTOR_SEARCH_ENDPOINT_NAME}. - this shouldn't happen: {endpoint}.\n Please delete it and re-run the previous cell: vsc.delete_endpoint("{VECTOR_SEARCH_ENDPOINT_NAME}")''')
else:
    raise Exception(f"Timeout, your endpoint isn't ready yet: {vsc.get_endpoint(VECTOR_SEARCH_ENDPOINT_NAME)}")


# COMMAND ----------

# MAGIC %sql
# MAGIC ALTER TABLE rag_chatbot_catalog.rag_chatbot_copen_new_data.copen_documentation_new_data SET TBLPROPERTIES (delta.enableChangeDataFeed = true);

# COMMAND ----------

# MAGIC %md
# MAGIC ## create index

# COMMAND ----------

# table for index
source_table_fullname = f"{catalog}.{db}.copen_documentation_new_data"
# index directory
vs_index_fullname = f"{catalog}.{db}.copen_new_documentation_vs_index"

# index check and status check
def check_index_and_wait(vsc, endpoint_name, index_full_name):
    try:
        idx = vsc.get_index(endpoint_name, index_full_name).describe()
        index_status = idx.get('status', idx.get('index_status', {}))
        status = index_status.get('detailed_state', index_status.get('status', 'UNKNOWN')).upper()
        
        # process when the index is exists
        if "ONLINE" in status:
            return True
        elif "UNKNOWN" in status:
            print(f"Can't get the status - will assume index is ready {idx} - url: {index_status.get('index_url', 'UNKNOWN')}")
            return True
        elif "PROVISIONING" in status:
            return False
        else:
            raise Exception(f'''Error with the index - this shouldn't happen. DLT pipeline might have been killed.\n Please delete it and re-run the previous cell: vsc.delete_index("{index_full_name}") \nIndex details: {idx}''')
    except Exception as e:
        if 'RESOURCE_DOES_NOT_EXIST' in str(e):
            return False
        else:
            print(f'Unexpected error describing the index. This could be a permission issue: {e}')
            raise e

# index check and create or sync
if not check_index_and_wait(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):
    print(f"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...")
    vsc.create_delta_sync_index(
        endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,
        index_name=vs_index_fullname,
        source_table_name=source_table_fullname,
        pipeline_type="TRIGGERED",
        primary_key="page",
        embedding_source_column='content',  # columns contains sentence
        embedding_model_endpoint_name=EMBEDDING_ENDPOINT_NAME  # endpoint when the handing endpoint is created
    )

    # all entities are created and the index is created
    for i in range(180):
        idx = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).describe()
        index_status = idx.get('status', idx.get('index_status', {}))
        status = index_status.get('detailed_state', index_status.get('status', 'UNKNOWN')).upper()
        
        if "ONLINE" in status:
            break
        elif "UNKNOWN" in status:
            print(f"Can't get the status - will assume index is ready {idx} - url: {index_status.get('index_url', 'UNKNOWN')}")
            break
        elif "PROVISIONING" in status:
            if i % 40 == 0:
                print(f"Waiting for index to be ready, this can take a few min... {index_status} - pipeline url:{index_status.get('index_url', 'UNKNOWN')}")
            time.sleep(10)
        else:
            raise Exception(f'''Error with the index - this shouldn't happen. DLT pipeline might have been killed.\n Please delete it and re-run the previous cell: vsc.delete_index("{vs_index_fullname}") \nIndex details: {idx}''')
    else:
        raise Exception(f"Timeout, your index isn't ready yet: {vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)}")

else:
    #  reload vector search and update index
    vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()

print(f"index on table is ready")


# COMMAND ----------

# URL from the serverless endpoint to make requests to the model
index_name=f"{catalog}.{db}.copen_new_documentation_vs_index"
host = "https://" + spark.conf.get("spark.databricks.workspaceUrl")
os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(SECRET_SCOPE_NAME, SECRET_SCOPE_KEY)

# COMMAND ----------

# embedding model
embedding_model = DatabricksEmbeddings(endpoint=EMBEDDING_ENDPOINT_NAME)

def get_retriever(persist_dir: str = None):
    os.environ["DATABRICKS_HOST"] = host
    # get vector search index
    vsc = VectorSearchClient(workspace_url=host, personal_access_token=os.environ["DATABRICKS_TOKEN"])
    vs_index = vsc.get_index(
        endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,
        index_name=index_name,
    )

    # create retriever
    vectorstore = DatabricksVectorSearch(
        vs_index, text_column="content", embedding=embedding_model, columns=["url"]
    )
    return vectorstore.as_retriever()

# COMMAND ----------

# MAGIC %md
# MAGIC ## create prompt

# COMMAND ----------

TEMPLATE = """私はダイハツ車であるコペンのためのアシスタントです。コペンの説明書に関連する質問に回答します。これらのトピックに関連しない質問の場合は、丁重に回答を辞退します。答えがわからない場合は、「わかりません」とだけ言います。回答はできるだけ簡潔な日本語にしてください。
最後に質問に回答するために以下のコンテキストのピースを使ってください:
{context}
質問: {question}
回答:
"""

chat_model = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, max_tokens = 4000)
prompt = PromptTemplate(template=TEMPLATE, input_variables=["context", "question"])

# create RAG chain
chain = RetrievalQA.from_chain_type(
    llm=chat_model,  # LMM chain
    chain_type="stuff",
    retriever=get_retriever(),
    return_source_documents=True,  # return sourse data
    chain_type_kwargs={"prompt": prompt}
)

# COMMAND ----------

# MAGIC %md
# MAGIC # Deploy

# COMMAND ----------

# create & update serving endpoint
mlflow.set_registry_uri("databricks-uc")
model_name = f"{catalog}.{db}.dbdemos_copen_chatbot_model"

def fetch_latest_model_version(model_identifier):
    client = MlflowClient(registry_uri="databricks-uc")
    max_version = 1
    
    for model_version in client.search_model_versions(f"name='{model_identifier}'"):
        current_version = int(model_version.version)
        if current_version > max_version:
            max_version = current_version
            
    return max_version

latest_model_version = fetch_latest_model_version(model_name)

w = WorkspaceClient()
endpoint_config = EndpointCoreConfigInput(
    name=SERVING_ENDPOINT_NAME,
    served_models=[
        ServedModelInput(
            model_name=model_name,
            model_version=latest_model_version,
            workload_size=ServedModelInputWorkloadSize.SMALL,
            scale_to_zero_enabled=True,
            environment_vars={
                "DATABRICKS_TOKEN": f"{{secrets/{SECRET_SCOPE_NAME}/{SECRET_SCOPE_KEY}}}",  # <scope>/<secret>
            }
        )
    ]
)

existing_endpoint = next(
    (e for e in w.serving_endpoints.list() if e.name == SERVING_ENDPOINT_NAME), None
)
serving_endpoint_url = f"{host}/ml/endpoints/{SERVING_ENDPOINT_NAME}"
if existing_endpoint == None:
    print(f"Creating the endpoint {serving_endpoint_url}, this will take a few minutes to package and deploy the endpoint...")
    w.serving_endpoints.create_and_wait(name=SERVING_ENDPOINT_NAME, config=endpoint_config)
else:
    # print(f"Updating the endpoint {serving_endpoint_url} to version {latest_model_version}, this will take a few minutes to package and deploy the endpoint...")
    # w.serving_endpoints.update_config_and_wait(served_models=endpoint_config.served_models, name=SERVING_ENDPOINT_NAME)
    pass

# COMMAND ----------

def front_url(port):
    ctx = get_context()
    proxy_url = f"https://{ctx.browserHostName}/driver-proxy/o/{ctx.workspaceId}/{ctx.clusterId}/{port}/"
    return proxy_url

PORT = 1502

# create HTML
displayHTML(f"<a href='{front_url(PORT)}' target='_blank' rel='noopener noreferrer'>別ウインドウで開く</a>")

streamlit_file = "streamlit.py"

# run streamlit
!streamlit run {streamlit_file} --server.port {PORT}

# COMMAND ----------

